# LLM

1.
results with hyperparameters:
batch_size = 64 
block_size = 256 
max_iters = 5000
eval_interval = 500
learning_rate = 3e-4
eval_iters = 200
n_embd = 384
n_head = 6
n_layer = 6
dropout = 0.2

and data split 90-10

Training loss=1.0553
Validation loss=1.4894

2.
results with hyperparameters:
batch_size = 64 
block_size = 256 
max_iters = 5000
eval_interval = 500
learning_rate = 3e-4
eval_iters = 200
n_embd = 384
n_head = 6
n_layer = 6
dropout = 0.3

and data split 85-15

Training loss=1.1126
Validation loss=1.6642


3.
results with hyperparameters:
batch_size = 64 
block_size = 256 
max_iters = 5000
eval_interval = 500
learning_rate = 3e-4
eval_iters = 200
n_embd = 256
n_head = 6
n_layer = 5
dropout = 0.4

and data split 85-15

Training loss=1.3320
Validation loss=1.7277

4.
results with hyperparameters:
batch_size = 64 
block_size = 256 
max_iters = 6000
eval_interval = 500
learning_rate = 2e-4
eval_iters = 200
n_embd = 384
n_head = 6
n_layer = 6
dropout = 0.35

and data split 85-15

Training loss=1.2401
Validation loss=1.6301
